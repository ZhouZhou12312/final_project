{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc30604-b0ec-450b-a19d-1a39c652afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform. python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46d82d-5e1d-4c3a-8511-383fc33c6687",
   "metadata": {},
   "source": [
    "### Some important notes here: the headlines begin with \"steps\" are required for user interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdfa9f-e7d5-4709-bcbd-7f01fe4e2a28",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf540c2e-2c7c-426c-8324-1b1a366f308e",
   "metadata": {},
   "source": [
    "So for this project, we need you to go to https://www.kaggle.com/datasets/saugataroyarghya/resume-dataset to download the dataset in this same file directory for ai training purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b35b3a-5170-48f1-89a6-b3b9cfbba551",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8599c608-5835-4933-853f-4e795d5fe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required imports for running the notebook, use pip to download if there is any missing notebook\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Optional dependencies for PDF/DOCX parsing\n",
    "try:\n",
    "    import PyPDF2\n",
    "except ImportError:\n",
    "    PyPDF2 = None\n",
    "\n",
    "try:\n",
    "    import docx\n",
    "except ImportError:\n",
    "    docx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec8a8a-904f-4a05-ac54-8efc7a6d3728",
   "metadata": {},
   "source": [
    "# 2. Data Collection and Preprocessing\n",
    "   - Upload user's own resume and job description file\n",
    "   - Data cleaning: Handling missing values, text preprocessing, and data labeling\n",
    "   - Save processed data to data/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3c7e9b-8dec-45d3-84c3-251419a2c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    DataPreprocessor handles loading raw resumes in multiple formats,\n",
    "    cleaning text, extracting structured sections, and saving processed output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dir: Directory path to scan by default or empty to prompt for paths\n",
    "            output_dir: Directory where processed JSON will be written.\n",
    "        \"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def _collect_paths(self, default_dir: str) -> list:\n",
    "        \"\"\"\n",
    "        Collect resume file paths from default_dir or via user input.\n",
    "        Accepts CSV, PDF, DOCX, TXT. If input is not a valid path, performs a fuzzy search\n",
    "        in the current working directory for matching filenames.\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        # 1) Scan default directory if it exists\n",
    "        if os.path.isdir(default_dir):\n",
    "            for ext in (\"*.csv\", \"*.pdf\", \"*.docx\", \"*.txt\"):\n",
    "                paths.extend(glob.glob(os.path.join(default_dir, ext)))\n",
    "        # 2) If still no files, prompt user with instructions\n",
    "        if not paths:\n",
    "            print(f\"No resume files found in '{default_dir}'.\")\n",
    "            print(\"Please enter either:\")\n",
    "            print(\"  • An absolute or relative file path (e.g. C:\\\\Users\\\\Rick\\\\Desktop\\\\resume.pdf or ~/resumes/resume.pdf)\")\n",
    "            print(\"  • A filename to fuzzy-search your workspace (e.g. 'resume.pdf')\")\n",
    "            user_input = input(\"Enter directory path, file path, or filename: \")\n",
    "            entries = [e.strip().strip('\"\\'') for e in user_input.split(',') if e.strip()]\n",
    "            for entry in entries:\n",
    "                entry_path = os.path.abspath(os.path.expanduser(entry))\n",
    "                if os.path.isdir(entry_path):\n",
    "                    for ext in (\"*.csv\", \"*.pdf\", \"*.docx\", \"*.txt\"):\n",
    "                        paths.extend(glob.glob(os.path.join(entry_path, ext)))\n",
    "                elif os.path.isfile(entry_path):\n",
    "                    paths.append(entry_path)\n",
    "                else:\n",
    "                    fuzzy = glob.glob(f\"**/{entry}\", recursive=True)\n",
    "                    if fuzzy:\n",
    "                        print(f\"Fuzzy match found for '{entry}': {fuzzy}\")\n",
    "                        paths.extend([os.path.abspath(p) for p in fuzzy])\n",
    "                    else:\n",
    "                        print(f\"Warning: '{entry}' not found or matched. Skipping.\")\n",
    "        return paths\n",
    "\n",
    "    def load_resumes(self) -> list:\n",
    "        \"\"\"\n",
    "        Load resumes from collected paths.\n",
    "        Returns a list of dicts: {'id': str, 'raw_text': str}.\n",
    "        \"\"\"\n",
    "        paths = self._collect_paths(self.input_dir)\n",
    "        if not paths:\n",
    "            raise RuntimeError(\"No valid resume file paths provided. Aborting.\")\n",
    "\n",
    "        records = []\n",
    "        for path in paths:\n",
    "            ext = os.path.splitext(path)[1].lower()\n",
    "            basename = os.path.basename(path)\n",
    "            try:\n",
    "                if ext == \".csv\":\n",
    "                    df = pd.read_csv(path)\n",
    "                    if \"resume_text\" not in df.columns:\n",
    "                        raise ValueError(f\"CSV '{basename}' is missing the 'resume_text' column.\")\n",
    "                    for idx, row in df.iterrows():\n",
    "                        records.append({\n",
    "                            'id': row.get('id', f\"{basename}_{idx}\"),\n",
    "                            'raw_text': str(row['resume_text'])\n",
    "                        })\n",
    "                elif ext == \".pdf\":\n",
    "                    if PyPDF2 is None:\n",
    "                        print(f\"Warning: PyPDF2 not installed; skipping PDF file '{basename}'.\")\n",
    "                        continue\n",
    "                    text_pages = []\n",
    "                    with open(path, \"rb\") as f:\n",
    "                        reader = PyPDF2.PdfReader(f)\n",
    "                        for page in reader.pages:\n",
    "                            text_pages.append(page.extract_text() or '')\n",
    "                    records.append({'id': basename, 'raw_text': '\\n'.join(text_pages)})\n",
    "                elif ext == \".docx\":\n",
    "                    if docx is None:\n",
    "                        print(f\"Warning: python-docx not installed; skipping DOCX file '{basename}'.\")\n",
    "                        continue\n",
    "                    document = docx.Document(path)\n",
    "                    paragraphs = [p.text for p in document.paragraphs]\n",
    "                    records.append({'id': basename, 'raw_text': '\\n'.join(paragraphs)})\n",
    "                elif ext == \".txt\":\n",
    "                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        records.append({'id': basename, 'raw_text': f.read()})\n",
    "                else:\n",
    "                    print(f\"Skipping unsupported file type: {basename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{basename}': {e}\")\n",
    "        return records\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text:\n",
    "          - lowercase\n",
    "          - remove non-ASCII characters\n",
    "          - collapse whitespace\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\x00-\\x7f]\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def extract_sections(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Placeholder for splitting text into sections like 'education', 'experience'.\n",
    "        TODO: implement via regex or NLP-based heading detection.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Full pipeline:\n",
    "          1) Load resumes\n",
    "          2) Clean text\n",
    "          3) Extract sections\n",
    "          4) Save processed data as JSON\n",
    "        \"\"\"\n",
    "        records = self.load_resumes()\n",
    "        processed = []\n",
    "        for rec in records:\n",
    "            clean = self.clean_text(rec['raw_text'])\n",
    "            sections = self.extract_sections(clean)\n",
    "            entry = {'id': rec['id'], 'clean_text': clean, **sections}\n",
    "            processed.append(entry)\n",
    "\n",
    "        out_path = os.path.join(self.output_dir, 'processed_resumes.json')\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Processed {len(processed)} resumes → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6795a-dbed-48a9-b947-31c466910c67",
   "metadata": {},
   "source": [
    "## Step1: Ask User to upload their Resume using required file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987ca510-e44e-4496-a99a-0c54244f8af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resume files found in 'data/raw'.\n",
      "Please enter either:\n",
      "  • An absolute or relative file path (e.g. C:\\Users\\Rick\\Desktop\\resume.pdf or ~/resumes/resume.pdf)\n",
      "  • A filename to fuzzy-search your workspace (e.g. 'resume.pdf')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter directory path, file path, or filename:  \"C:\\Users\\Rick\\Desktop\\5293\\final_project_4\\Jack_TotallyUnqualified_Resume.pdf\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 resumes → data/processed\\processed_resumes.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_dir = 'data/raw'\n",
    "    output_dir = 'data/processed'\n",
    "    dp = DataPreprocessor(input_dir, output_dir)\n",
    "    dp.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278406bc-b4ea-47e3-8bbd-5bade6358131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Input job description (supports .txt, .pdf, .docx, or URL)\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "# Function to load job description from file or URL\n",
    "def load_job_description(source: str) -> str:\n",
    "    \"\"\"\n",
    "    Load job description text from a local file (.txt|.pdf|.docx) or URL.\n",
    "    Performs fuzzy substring search if the direct path fails.\n",
    "    \"\"\"\n",
    "    source = source.strip().strip('\"\\'')\n",
    "    # URL\n",
    "    if source.startswith(('http://', 'https://')):\n",
    "        import requests\n",
    "        r = requests.get(source)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    # Local\n",
    "    path = os.path.abspath(os.path.expanduser(source))\n",
    "    if os.path.isfile(path):\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext == '.txt':\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        elif ext == '.pdf':\n",
    "            from PyPDF2 import PdfReader\n",
    "            reader = PdfReader(path)\n",
    "            return \"\\n\".join([page.extract_text() or '' for page in reader.pages])\n",
    "        elif ext == '.docx':\n",
    "            import docx\n",
    "            doc = docx.Document(path)\n",
    "            return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported extension: {ext}\")\n",
    "    # Fuzzy search\n",
    "    name = os.path.basename(source)\n",
    "    print(f\"'{source}' not found; fuzzy searching for '*{name}*'...\")\n",
    "    matches = glob.glob(f\"**/*{name}*\", recursive=True)\n",
    "    if matches:\n",
    "        print(f\"Found: {matches[0]}\")\n",
    "        return load_job_description(matches[0])\n",
    "    raise FileNotFoundError(f\"Source '{source}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6a791-43b4-4070-93bc-f96e9ce0a55c",
   "metadata": {},
   "source": [
    "## Step2: Ask User to put their own job description with required format needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875cafde-c6ce-4dd4-82db-4aecc956e843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter job description (.txt/.pdf/.docx path or URL):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \"C:\\Users\\Rick\\Desktop\\5293\\final_project_4\\sample-job-description.pdf\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEnter job description (.txt/.pdf/.docx path or URL):\")\n",
    "src = input()\n",
    "job_description = load_job_description(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31246349-ab6a-49ca-9d82-52bd57485ea3",
   "metadata": {},
   "source": [
    "# 3. Model Selection and Training\n",
    "   - Choose model architecture: GPT-2 from hugging face\n",
    "   - Fine-tune the model using resume and job description data from Kaggle datasets\n",
    "   - Implement prompt optimization strategies to refine output quality\n",
    "   - Save the trained model to models/fine_tuned_llm.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56556834-07f2-4c54-a008-ad070174885f",
   "metadata": {},
   "source": [
    "## Step3: Ask User to enter their own hugging face token key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6645bbf-6664-4d2a-be31-7a07e3057276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face access token:  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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "\n",
    "# 1) Prompt user for Hugging Face access token\n",
    "hf_token = input(\"Enter your Hugging Face access token: \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8febfa27-49f0-45c7-bbed-8e0b1e3dde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Paths\n",
    "DATA_PATH = \"resume_data.csv\"\n",
    "OUTPUT_DIR = \"flan-t5-base-resume\"\n",
    "\n",
    "# 2) Load and preprocess CSV\n",
    "df = pd.read_csv(DATA_PATH, encoding=\"utf-8\")\n",
    "df.rename(columns=lambda x: x.lstrip(\"\\ufeff\"), inplace=True)\n",
    "\n",
    "def assemble_resume(row):\n",
    "    parts = []\n",
    "    if pd.notna(row.get(\"career_objective\")):\n",
    "        parts.append(row[\"career_objective\"])\n",
    "    if pd.notna(row.get(\"skills\")):\n",
    "        parts.append(\"Skills: \" + row[\"skills\"])\n",
    "    if pd.notna(row.get(\"responsibilities\")):\n",
    "        parts.append(\"Responsibilities: \" + row[\"responsibilities\"])\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def assemble_jd(row):\n",
    "    parts = []\n",
    "    if pd.notna(row.get(\"job_position_name\")):\n",
    "        parts.append(\"Position: \" + row[\"job_position_name\"])\n",
    "    if pd.notna(row.get(\"skills_required\")):\n",
    "        parts.append(\"Required Skills: \" + row[\"skills_required\"])\n",
    "    if pd.notna(row.get(\"responsibilities.1\")):\n",
    "        parts.append(\"Responsibilities: \" + row[\"responsibilities.1\"])\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "df[\"resume_text\"] = df.apply(assemble_resume, axis=1)\n",
    "df[\"jd_text\"]     = df.apply(assemble_jd, axis=1)\n",
    "df = df[\n",
    "    df[\"resume_text\"].str.strip().astype(bool) &\n",
    "    df[\"jd_text\"].str.strip().astype(bool)\n",
    "]\n",
    "\n",
    "# 3) Build a Hugging Face Dataset\n",
    "df[\"prompt\"]     = df[\"jd_text\"] + \"\\n\\nPlease generate a resume based on the above JD:\"\n",
    "df[\"completion\"] = df[\"resume_text\"]\n",
    "hf_ds = Dataset.from_pandas(df[[\"prompt\", \"completion\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b8b7c7-7382-4175-9123-4e6a94a59cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23427006e1146b7b37ea3e954916a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: zz3237 (zz3237-columbia-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Rick\\Desktop\\5293\\final_project_4\\wandb\\run-20250513_201659-m3vu4yov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zz3237-columbia-university/huggingface/runs/m3vu4yov' target=\"_blank\">flan-t5-tinytest</a></strong> to <a href='https://wandb.ai/zz3237-columbia-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zz3237-columbia-university/huggingface' target=\"_blank\">https://wandb.ai/zz3237-columbia-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zz3237-columbia-university/huggingface/runs/m3vu4yov' target=\"_blank\">https://wandb.ai/zz3237-columbia-university/huggingface/runs/m3vu4yov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:18, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.551200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills: Data Analyst (Requirement: Python, SQL)\n"
     ]
    }
   ],
   "source": [
    "small_ds = hf_ds.shuffle(seed=42).select(range(200))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-base\",\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    inp = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    tgt = tokenizer(\n",
    "        examples[\"completion\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inp[\"labels\"] = tgt[\"input_ids\"]\n",
    "    return inp\n",
    "\n",
    "tokenized_ds = small_ds.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"completion\"]\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"flan-t5-tinytest\",\n",
    "    per_device_train_batch_size=16,\n",
    "    max_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Make sure this path matches the one you use in inference\n",
    "OUTPUT_DIR = \"flan-t5-base-resume\"  \n",
    "\n",
    "\n",
    "out = model.generate(**tokenizer(\n",
    "    \"Position: Data Analyst\\nRequired Skills: Python, SQL\\n\\nPlease generate a resume:\",\n",
    "    return_tensors=\"pt\"\n",
    "))\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb9def-8a93-4c78-8811-389a10ae11a4",
   "metadata": {},
   "source": [
    "# 4. Generate modified Resume based on fine-tuned gpt model from above and generate a cover letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78539309-1de6-4f1c-8431-67a8ca63ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Outputs (one resume + cover letter) saved under data/final_outputs/\n"
     ]
    }
   ],
   "source": [
    "# === Section 4: Inference on processed resumes + user’s JD ===\n",
    "\n",
    "import os, json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 1) Load the processed resumes JSON\n",
    "proc_path = \"data/processed/processed_resumes.json\"\n",
    "with open(proc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    processed = json.load(f)\n",
    "\n",
    "if not processed:\n",
    "    raise RuntimeError(\"No processed resumes found at \" + proc_path)\n",
    "\n",
    "# 2) Load your fine‐tuned Flan-T5 checkpoint\n",
    "MODEL_DIR = \"flan-t5-base-resume\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# 3) Prepare output folder\n",
    "out_dir = \"data/final_outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 4) Loop through each processed resume\n",
    "for rec in processed:\n",
    "    rec_id       = rec[\"id\"]\n",
    "    clean_resume = rec[\"clean_text\"]\n",
    "    jd           = job_description   # from Section 2\n",
    "\n",
    "    # ——— a) Rewrite & expand resume ———\n",
    "    resume_prompt = (\n",
    "        f\"{jd}\\n\\n\"\n",
    "        \"Here is my current resume:\\n\"\n",
    "        f\"{clean_resume}\\n\\n\"\n",
    "        \"Please rewrite and expand my resume to better match the job description above:\"\n",
    "    )\n",
    "    inputs = tokenizer(\n",
    "        resume_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    out_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=250,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    new_resume = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # ——— b) Generate cover letter ———\n",
    "    cover_prompt = (\n",
    "        f\"{jd}\\n\\n\"\n",
    "        \"Write a concise, enthusiastic cover letter highlighting my skills \"\n",
    "        \"and experience as shown above, tailored to this job description:\"\n",
    "    )\n",
    "    inputs2 = tokenizer(\n",
    "        cover_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    out2_ids = model.generate(\n",
    "        inputs2.input_ids,\n",
    "        attention_mask=inputs2.attention_mask,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    cover_letter = tokenizer.decode(out2_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # ——— c) Save results ———\n",
    "    with open(f\"{out_dir}/{rec_id}_resume.txt\",       \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(new_resume)\n",
    "    with open(f\"{out_dir}/{rec_id}_cover_letter.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cover_letter)\n",
    "\n",
    "    # remove this break to process *all* uploads; kept here to demo just the first\n",
    "    break\n",
    "\n",
    "print(f\"Done. Outputs (one resume + cover letter) saved under {out_dir}/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1769c-5413-48fe-9b55-288e9edfdb1c",
   "metadata": {},
   "source": [
    "# Redo part 3 and 4 for changing training logic\n",
    "\n",
    "### By precomputing embeddings for our JD–resume pairs and using FAISS to retrieve just three examples for a few-shot prompt to flan-t5-small, we eliminated all fine-tuning and achieve CPU-only, sub-second resume and cover letter generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff4ea04-ea05-48e4-b962-0665c6ff4503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Computing embeddings in batches…\n",
      " Indexed 9544 examples. Ready for few-shot retrieval.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv(\"resume_data.csv\", encoding=\"utf-8\")\n",
    "df.rename(columns=lambda x: x.lstrip(\"\\ufeff\"), inplace=True)\n",
    "\n",
    "def assemble_resume(r):\n",
    "    parts = []\n",
    "    if pd.notna(r.get(\"career_objective\")):\n",
    "        parts.append(r[\"career_objective\"])\n",
    "    if pd.notna(r.get(\"skills\")):\n",
    "        parts.append(\"Skills: \" + r[\"skills\"])\n",
    "    if pd.notna(r.get(\"responsibilities\")):\n",
    "        parts.append(\"Responsibilities: \" + r[\"responsibilities\"])\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def assemble_jd(r):\n",
    "    parts = []\n",
    "    if pd.notna(r.get(\"job_position_name\")):\n",
    "        parts.append(\"Position: \" + r[\"job_position_name\"])\n",
    "    if pd.notna(r.get(\"skills_required\")):\n",
    "        parts.append(\"Required Skills: \" + r[\"skills_required\"])\n",
    "    if pd.notna(r.get(\"responsibilities.1\")):\n",
    "        parts.append(\"Responsibilities: \" + r[\"responsibilities.1\"])\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "df[\"resume_text\"] = df.apply(assemble_resume, axis=1)\n",
    "df[\"jd_text\"]     = df.apply(assemble_jd, axis=1)\n",
    "\n",
    "df = df[\n",
    "    df[\"resume_text\"].str.strip().astype(bool) &\n",
    "    df[\"jd_text\"].str.strip().astype(bool)\n",
    "]\n",
    "\n",
    "prompts     = df[\"jd_text\"].tolist()\n",
    "completions = df[\"resume_text\"].tolist()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "embed_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embed_tok   = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name).eval()\n",
    "\n",
    "def compute_embeddings(texts, batch_size=32):\n",
    "    all_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i+batch_size]\n",
    "        inputs = embed_tok(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = embed_model(**inputs)\n",
    "        # mean pooling\n",
    "        embs = outputs.last_hidden_state.mean(dim=1)\n",
    "        all_embs.append(embs.cpu().numpy())\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "prompts = df[\"jd_text\"].tolist()\n",
    "completions = df[\"resume_text\"].tolist()\n",
    "\n",
    "print(\" Computing embeddings in batches…\")\n",
    "embs = compute_embeddings(prompts)\n",
    "\n",
    "import faiss\n",
    "dim   = embs.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embs)\n",
    "\n",
    "print(f\" Indexed {embs.shape[0]} examples. Ready for few-shot retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85def53e-6ab5-4579-aa86-466930372f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done—check data/final_outputs/ for your new resumes & cover letters.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "with open(\"data/processed/processed_resumes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    processed = json.load(f)\n",
    "\n",
    "MODEL_DIR = \"flan-t5-base-resume\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(\"data/final_outputs\", exist_ok=True)\n",
    "\n",
    "for rec in processed:\n",
    "    rid  = rec[\"id\"]\n",
    "    res  = rec[\"clean_text\"]\n",
    "    jd   = job_description\n",
    "\n",
    "    # a) resume\n",
    "    prompt = (\n",
    "      f\"### JD:\\n{jd}\\n\\n\"\n",
    "      \"### Current Resume:\\n\" + res + \"\\n\\n\"\n",
    "      \"### Rewritten Resume:\\n\"\n",
    "      \"- Professional Summary (2–3 sentences)\\n\"\n",
    "      \"- Key Skills (bullet list)\\n\"\n",
    "      \"- Experience (bullets, quantifiable)\\n\"\n",
    "      \"- Education\\n\"\n",
    "    )\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                    max_length=512, padding=\"longest\")\n",
    "    out = model.generate(\n",
    "      **enc, max_new_tokens=250,\n",
    "      do_sample=True, temperature=0.7, top_p=0.9,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    new_resume = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # b) Cover letter\n",
    "    prompt2 = (\n",
    "      f\"### JD:\\n{jd}\\n\\n\"\n",
    "      \"### Resume:\\n\" + res + \"\\n\\n\"\n",
    "      \"### Cover Letter (3 paragraphs):\\n\"\n",
    "      \"1. Intro (position, company)\\n\"\n",
    "      \"2. Highlight 2–3 key achievements\\n\"\n",
    "      \"3. Closing (enthusiasm, next steps)\\n\"\n",
    "    )\n",
    "    enc2 = tokenizer(prompt2, return_tensors=\"pt\", truncation=True,\n",
    "                     max_length=512, padding=\"longest\")\n",
    "    out2 = model.generate(\n",
    "      **enc2, max_new_tokens=300,\n",
    "      do_sample=True, temperature=0.7, top_p=0.9,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    cover = tokenizer.decode(out2[0], skip_special_tokens=True)\n",
    "\n",
    "    # c) Save\n",
    "    with open(f\"data/final_outputs/{rid}_resume.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(new_resume)\n",
    "    with open(f\"data/final_outputs/{rid}_cover_letter.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cover)\n",
    "\n",
    "    break\n",
    "\n",
    "print(\"Done—check data/final_outputs/ for your new resumes & cover letters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea75161a-9ac4-436f-9945-4732a3cd7300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPDATED RESUME ===\n",
      " i want to be a risk manager.\n",
      "\n",
      "=== COVER LETTER ===\n",
      " monopoly)\n"
     ]
    }
   ],
   "source": [
    "for rec in processed:\n",
    "    rid  = rec[\"id\"]\n",
    "    res  = rec[\"clean_text\"]\n",
    "    jd   = job_description\n",
    "\n",
    "    resume_prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Rewrite and expand the candidate’s resume to match the job description below.\\n\"\n",
    "        \"Output **four** clearly-labeled sections. Do **not** include anything else.\\n\\n\"\n",
    "\n",
    "        \"### Job Description:\\n\"\n",
    "        f\"{jd}\\n\\n\"\n",
    "\n",
    "        \"### Current Resume:\\n\"\n",
    "        f\"{res}\\n\\n\"\n",
    "\n",
    "        \"### Updated Resume:\\n\"\n",
    "        \"1) Professional Summary (2–3 sentences)\\n\"\n",
    "        \"2) Key Skills (bullet list, max 6 items)\\n\"\n",
    "        \"3) Experience (bullet list, focus on quantifiable achievements, max 5 items)\\n\"\n",
    "        \"4) Education (degree, institution, year)\\n\\n\"\n",
    "        \"Begin your answer **immediately after** \\\"### Updated Resume:\\\"\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        resume_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, max_length=512, padding=\"longest\"\n",
    "    ).to(\"cpu\")\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True, temperature=0.7, top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    new_resume = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    cover_prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Write a **three-paragraph** cover letter based on the job description & resume below.\\n\"\n",
    "        \"Do **not** include any preamble, explanations, or repeat the JD. Output **only** the letter.\\n\\n\"\n",
    "\n",
    "        \"### Job Description:\\n\"\n",
    "        f\"{jd}\\n\\n\"\n",
    "\n",
    "        \"### Resume:\\n\"\n",
    "        f\"{res}\\n\\n\"\n",
    "\n",
    "        \"### Cover Letter:\\n\"\n",
    "        \"Paragraph 1: Introduce application, mention position & company.\\n\"\n",
    "        \"Paragraph 2: Highlight 2–3 relevant achievements from the resume.\\n\"\n",
    "        \"Paragraph 3: Express enthusiasm and next steps.\\n\\n\"\n",
    "        \"Begin your answer **immediately after** \\\"### Cover Letter:\\\"\"\n",
    "    )\n",
    "\n",
    "    inputs2 = tokenizer(\n",
    "        cover_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, max_length=512, padding=\"longest\"\n",
    "    ).to(\"cpu\")\n",
    "    out2_ids = model.generate(\n",
    "        **inputs2,\n",
    "        max_new_tokens=350,\n",
    "        do_sample=True, temperature=0.7, top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    cover_letter = tokenizer.decode(out2_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    print(\"=== UPDATED RESUME ===\\n\", new_resume)\n",
    "    print(\"\\n=== COVER LETTER ===\\n\", cover_letter)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
